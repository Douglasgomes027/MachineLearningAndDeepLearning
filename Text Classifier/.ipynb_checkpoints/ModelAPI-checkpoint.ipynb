{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@app.route(\"/\")\n",
    "def hello_world():\n",
    "    return \"Hello World! <strong>I am learning Flask</strong>\", 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'model_v1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filename ,'rb') as f:\n",
    "    loaded_model = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACESSORIO DE INFORMATICA' 'ACESSORIO DE VESTUARIO' 'ACESSORIO DE VIAGEM'\n",
      " 'ACESSORIO E EQUIPAMENTO PARA ESPORTE' 'ACESSORIO ELETRONICO'\n",
      " 'ACESSORIO MUSICAL' 'ACESSORIO PARA BEBE' 'ACESSORIO PARA BELEZA'\n",
      " 'ACESSORIO PARA CALCADO' 'ACESSORIO PARA CELULAR'\n",
      " 'ACESSORIO PARA TELEFONE FIXO' 'ACESSORIO PARA VEICULO'\n",
      " 'ACESSORIOS RELIGIOSOS' 'ACUCAR' 'ADOCANTE' 'ADUBO' 'AGUA' 'AGUARDENTE'\n",
      " 'ALARME' 'ALIMENTO CONDIMENTADO' 'ALIMENTO EM CONSERVA'\n",
      " 'ALIMENTO PARA ANIMAL' 'ALIMENTO PRONTO' 'ALUMINIO' 'APERITIVO' 'ARROZ'\n",
      " 'ARTIGO PARA DECORACAO' 'AVIAMENTO' 'BIJUTERIA' 'BISCOITO' 'BOMBONIERE'\n",
      " 'BOTA' 'BRINQUEDO' 'CABELO' 'CAFE' 'CAJUINA' 'CAMA MESA E BANHO'\n",
      " 'CAMERA E FILMADORA' 'CARNE BOVINA' 'CARNE DE AVE ' 'CARNE DE CRUSTACEO'\n",
      " 'CARNE DE PEIXE' 'CARNE OVINA' 'CARNE SUINA' 'CARTOES' 'CASCA DE FRUTA'\n",
      " 'CELULAR' 'CEREAL' 'CERVEJA' 'CEVADA' 'CHA' 'CHAMPAGNE' 'CHAPA DE MADEIRA'\n",
      " 'CHINELO' 'COMBUSTIVEL' 'CORPO' 'DOCE E SOBREMESA' 'ELETRODOMESTICO'\n",
      " 'ELETRONICO' 'ELETROPORTATEIS' 'EMBALAGEM PARA PRESENTE' 'ENERGETICO'\n",
      " 'EQUIPAMENTO HOSPITALAR' 'EROTICO' 'ESPECIARIA' 'ESPUMANTE'\n",
      " 'ESTRUTURA FLUTUANTE' 'EXCLUIDOS' 'FAIXAS' 'FARELO' 'FARINHA E AMIDO'\n",
      " 'FAX E MINI CENTRAL' 'FEIJAO' 'FERRAGEM' 'FERTILIZANTE' 'FOLHETO'\n",
      " 'FORMAS DE TABACO' 'FRUTA' 'GAME' 'GENERICO' 'GRANITO' 'HIDROTONICO'\n",
      " 'INFORMATICA' 'INSTRUMENTO DE CORDA' 'INSTRUMENTO DE PERCUSSAO'\n",
      " 'ISOTONICO' 'JORNAL' 'LABIOS' 'LACTICINIO' 'LEITE' 'LICOR' 'LIVRO'\n",
      " 'MACARRAO' 'MAOS' 'MAQUIAGEM' 'MAQUINA E EQUIPAMENTO' 'MARMORE'\n",
      " 'MASSA PRONTA' 'MATERIAL ELETRICO' 'MATERIAL PARA AGROPECUARIA'\n",
      " 'MATERIAL PARA ARTESANATO' 'MATERIAL PARA CONSTRUCAO'\n",
      " 'MATERIAL PARA ESCRITORIO E ESCOLAR' 'MEDICAMENTO NAO CLASSIFICADO' 'MEL'\n",
      " 'MISTURA PRONTA' 'MODA PRAIA' 'ODONTOLOGICO' 'OLEO' 'OLHOS' 'ORTOPEDIA'\n",
      " 'OUTROS MEDICAMENTOS' 'OVO DE AVE' 'PAO' 'PAPEL'\n",
      " 'PARTE E ACESSORIO DE RELOJOARIA' 'PARTE E PECA PARA VEICULO'\n",
      " 'PECA INTIMA' 'PECAS E ACESSORIOS PARA MOVEIS' 'PEDRA CARIRI' 'PERFUMARIA'\n",
      " 'PES' 'PREPARACAO DE CARNE' 'PREPARACAO DE FRUTA' 'PRODUTO DE CONFEITARIA'\n",
      " 'PRODUTO DE FLORICULTURA' 'PRODUTO DE HIGIENE' 'PRODUTO DE LIMPEZA'\n",
      " 'PRODUTO DESCARTAVEL' 'PRODUTO FARMACEUTICO' 'PRODUTO HOSPITALAR'\n",
      " 'PRODUTO OTICO' 'PRODUTO PARA ANIMAL' 'PRODUTO PARA CONSERVACAO'\n",
      " 'PRODUTO PARA FESTA' 'PRODUTO PARA ORGANIZACAO' 'PRODUTO PARA SEGURANCA'\n",
      " 'PRODUTO PARA SOLDA' 'PRODUTO VEGETAL' 'PROTEINA DE SOJA' 'REFERENCIA'\n",
      " 'REFRIGERANTE' 'RELOGIO' 'REVISTA' 'ROSTO' 'RUM' 'SANDALIA' 'SAPATILHA'\n",
      " 'SAPATO' 'SAQUE' 'SIMILAR' 'SUCATA' 'SUCO' 'TELEFONE FIXO' 'TEMPERO'\n",
      " 'TENIS' 'TEQUILA' 'TINTA PARA TECIDO' 'TIPO DE FERRAMENTA'\n",
      " 'TIPO DE FOSFORO' 'TIPO DE GRAO' 'TIPO DE MADEIRA' 'TIPO DE MOVEIS'\n",
      " 'TIPO DE PESTICIDA' 'TIPO DE PLANTA' 'TIPO DE PRODUTO' 'TIPO DE SEMENTE'\n",
      " 'TIPO DE TABACO' 'TIPO DE TECIDO' 'TIPO DE VELA' 'TIPO DE VIDRO'\n",
      " 'TOUCINHO E GORDURA SUINA E DE AVE' 'UNHAS' 'UTENSILIO DE LIMPEZA'\n",
      " 'UTENSILIO PARA BEBE' 'UTENSILIO PARA O LAR' 'VEICULO TERRESTRE'\n",
      " 'VERDURA E LEGUME' 'VERMOUTH' 'VESTUARIO FEMININO' 'VESTUARIO INFANTIL'\n",
      " 'VESTUARIO MASCULINO' 'VESTUARIO PARA BEBE' 'VINHO' 'VODKA' 'WHISKY']\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_train = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fd3f9aae13ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mteste_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPSON MAGENTO'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'LILLO BICO'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'BOV AMERICANA KG'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'KING 100ML JASMIM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mteste_predict_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteste_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteste_predict_vect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "teste_predict=['EPSON MAGENTO','LILLO BICO','BOV AMERICANA KG','KING 100ML JASMIM']\n",
    "teste_predict_vect = vectorizer_train.transform(teste_predict) \n",
    "loaded_model.predict(teste_predict_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teste_predict_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
